{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import openai\n",
    "from openai.error import Timeout as OpenAITimeout\n",
    "import pandas as pd\n",
    "from pandas.io.sql import DatabaseError  # type: ignore\n",
    "from requests.exceptions import Timeout as RequestsTimeout\n",
    "import sqlglot\n",
    "from sqlglot import expressions, Expression\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "SET = \"bird-data-train\"\n",
    "JSON_FILE = \"./\" + SET + \"/train.json\"\n",
    "DB_DIR = \"./\" + SET + \"/train_databases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokens(row):\n",
    "    msgs = [{\"role\": \"user\", \"content\": row['user_prompt']}]\n",
    "    return num_tokens_from_messages(msgs)\n",
    "\n",
    "def get_create_table_and_data(db_path: str, num_rows: int = 5) -> str:\n",
    "    MAX_TOKENS = 3550 # The limit required so OpenAI doesn't complain after we reformat\n",
    "    while num_rows >= 0:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path, timeout=30)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Query the sqlite_master table to get the CREATE TABLE statements\n",
    "        cursor.execute(\n",
    "            \"SELECT name, sql FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\"\n",
    "        )\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        output_statements = []\n",
    "\n",
    "        for table_name, create_statement in tables:\n",
    "            # \"INTEGER\" -> \"INT\"\n",
    "            create_statement = create_statement.replace(\"INTEGER\", \"INT\")\n",
    "            \n",
    "            # remove comments\n",
    "            create_statement = re.sub(r'--.*$', '', create_statement, flags=re.MULTILINE)\n",
    "            create_statement = \"\\n\".join([line for line in create_statement.split(\"\\n\") if line.strip()])\n",
    "\n",
    "            # Condense whitespace\n",
    "            create_statement = \" \".join(create_statement.split())\n",
    "            \n",
    "            # First, add the create statement\n",
    "            output_statements.append(create_statement + \";\")\n",
    "\n",
    "            # Fetch sample data\n",
    "            cursor.execute(f\"SELECT * FROM `{table_name}` LIMIT ?\", (num_rows,))\n",
    "            sample_rows = cursor.fetchall()\n",
    "\n",
    "            # For each row, create an INSERT INTO statement\n",
    "            for row in sample_rows:\n",
    "                formatted_values = []\n",
    "                for idx, value in enumerate(row):\n",
    "                    if isinstance(value, str):\n",
    "                        formatted_value = value.replace('\\n', ' ')\n",
    "                        formatted_value = formatted_value.replace(\"'\", '\"')\n",
    "                        formatted_value = formatted_value[:100]\n",
    "                        formatted_values.append(f\"'{formatted_value}'\")\n",
    "                    elif value is None:\n",
    "                        formatted_values.append(\"NULL\")\n",
    "                    else:\n",
    "                        formatted_values.append(str(value))\n",
    "                values_str = \",\".join(formatted_values)\n",
    "            \n",
    "                # Check if table_name contains a space or dash and wrap it in double quotes if it does\n",
    "                if ' ' in table_name or '-' in table_name:\n",
    "                    formatted_table_name = f'\"{table_name}\"'\n",
    "                else:\n",
    "                    formatted_table_name = table_name\n",
    "\n",
    "                insert_statement = f\"INSERT INTO {formatted_table_name} VALUES ({values_str});\"\n",
    "                output_statements.append(insert_statement)\n",
    "\n",
    "        # Close the database connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        final_statements = []\n",
    "        for statement in output_statements:\n",
    "            test_statements = final_statements + [statement]\n",
    "            msgs = [{\"role\": \"user\", \"content\": \"\\n\".join(test_statements)}]\n",
    "            token_count = num_tokens_from_messages(msgs)\n",
    "\n",
    "            if token_count < MAX_TOKENS:\n",
    "                final_statements = test_statements\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if num_rows == 0 or final_statements:\n",
    "            return final_statements\n",
    "        else:\n",
    "            num_rows -= 1\n",
    "    raise ValueError(f\"Even with 0 rows, token count is too high!\")\n",
    "\n",
    "\n",
    "def clean_creates(sql_text: str) -> str:\n",
    "    \"\"\"While these fields might be useful for some purposes, I've honestly\n",
    "    needed them so rarely as a data scientist that we are going to exclude them\n",
    "    \"\"\"\n",
    "\n",
    "    def replace_(node: Expression) -> Optional[Expression]:\n",
    "        if isinstance(\n",
    "            node,\n",
    "            (\n",
    "                expressions.ColumnConstraint,\n",
    "                # expressions.ForeignKey,\n",
    "                expressions.PrimaryKey,\n",
    "            ),\n",
    "        ):\n",
    "            return None\n",
    "        return node\n",
    "    return str(sqlglot.parse_one(sql_text).transform(replace_))\n",
    "\n",
    "\n",
    "def hard_replace__clean_creates(sql_text: str):\n",
    "    \"\"\"The backticks and double-quotes are always equivalent in bird\n",
    "    # but sqlglot cannot yet handle the backticks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return clean_creates(\n",
    "            sql_text.replace(\"`\", '\"')\n",
    "            .replace(\"WITHOUT ROWID\", \"\")\n",
    "            .replace(\"on update cascade\",\"\")\n",
    "            .replace(\"ON UPDATE CASCADE\",\"\")\n",
    "            .replace(\"on delete cascade\",\"\")\n",
    "            .replace(\"ON DELETE CASCADE\",\"\")\n",
    "            .replace(\"references staff\",\"\")\n",
    "        )  # .sql()\n",
    "    except Exception:\n",
    "        print(sql_text)\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_in_all_sqlite_dbs():\n",
    "    \"\"\"Read in all the sqlite databases from the bird data\"\"\"\n",
    "    dirs = glob.glob(DB_DIR + \"/*\")\n",
    "    statements = []\n",
    "    for d in dirs:\n",
    "        if os.path.isfile(d):\n",
    "            continue\n",
    "        dbname = d.split(\"/\")[-1]\n",
    "        sqlite_db_path = os.path.join(d, dbname + \".sqlite\")\n",
    "        assert os.path.exists(d), f\"DB {d} does not exist!\"\n",
    "        ddl_list = get_create_table_and_data(sqlite_db_path)\n",
    "        for ddl in ddl_list:\n",
    "            statements.append(\n",
    "                (dbname, hard_replace__clean_creates(ddl))\n",
    "            )\n",
    "\n",
    "    return statements\n",
    "\n",
    "\n",
    "def make_x(tables, db_id, ideal_sql):\n",
    "    \"\"\"Make the x and y for the training data\"\"\"\n",
    "    return tables, db_id, ideal_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>ddl</th>\n",
       "      <th>question</th>\n",
       "      <th>evidence</th>\n",
       "      <th>SQL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>address</td>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>What is the total number of households in Arec...</td>\n",
       "      <td>\"ARECIBO\" is the county; total number of house...</td>\n",
       "      <td>SELECT SUM(T1.households) FROM zip_data AS T1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>address</td>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>Which residential area in Arecibo county has t...</td>\n",
       "      <td>\"ARECIBO\" is the county; highest average house...</td>\n",
       "      <td>SELECT T1.zip_code FROM zip_data AS T1 INNER J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>address</td>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>Please list the numbers of males in all the re...</td>\n",
       "      <td>\"ARECIBO\" is the county; number of males refer...</td>\n",
       "      <td>SELECT SUM(T1.male_population) FROM zip_data A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>address</td>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>Among all the residential areas in Delaware, h...</td>\n",
       "      <td>\"Delaware\" is a county; implement daylight sav...</td>\n",
       "      <td>SELECT COUNT(T1.zip_code) FROM zip_data AS T1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>address</td>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>Among all the residential areas in Arecibo cou...</td>\n",
       "      <td>\"ARECIBO\" is the county; highest white populat...</td>\n",
       "      <td>SELECT T1.zip_code FROM zip_data AS T1 INNER J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     db_id                                                ddl  \\\n",
       "0  address  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...   \n",
       "1  address  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...   \n",
       "2  address  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...   \n",
       "3  address  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...   \n",
       "4  address  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What is the total number of households in Arec...   \n",
       "1  Which residential area in Arecibo county has t...   \n",
       "2  Please list the numbers of males in all the re...   \n",
       "3  Among all the residential areas in Delaware, h...   \n",
       "4  Among all the residential areas in Arecibo cou...   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  \"ARECIBO\" is the county; total number of house...   \n",
       "1  \"ARECIBO\" is the county; highest average house...   \n",
       "2  \"ARECIBO\" is the county; number of males refer...   \n",
       "3  \"Delaware\" is a county; implement daylight sav...   \n",
       "4  \"ARECIBO\" is the county; highest white populat...   \n",
       "\n",
       "                                                 SQL  \n",
       "0  SELECT SUM(T1.households) FROM zip_data AS T1 ...  \n",
       "1  SELECT T1.zip_code FROM zip_data AS T1 INNER J...  \n",
       "2  SELECT SUM(T1.male_population) FROM zip_data A...  \n",
       "3  SELECT COUNT(T1.zip_code) FROM zip_data AS T1 ...  \n",
       "4  SELECT T1.zip_code FROM zip_data AS T1 INNER J...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddl_statements = read_in_all_sqlite_dbs()\n",
    "\n",
    "df_ddl = pd.DataFrame(ddl_statements)\n",
    "df_ddl.columns = [\"db_id\", \"ddl\"]\n",
    "df_ddl = df_ddl.groupby(\"db_id\")[\"ddl\"].agg(lambda x: \"\\n\".join(x)).reset_index(name=\"ddl\")\n",
    "\n",
    "def format_ddl(ddl_str):\n",
    "    formatted_ddls = []\n",
    "\n",
    "    # Split the ddl_str by \"CREATE TABLE\"\n",
    "    create_tables = re.split(r'(?i)CREATE TABLE', ddl_str)\n",
    "\n",
    "    for ct in create_tables:\n",
    "        if not ct.strip():\n",
    "            continue\n",
    "\n",
    "        # Extract table name from the current CREATE TABLE section\n",
    "        table_name_match = re.search(r'^\\s*(\"?[\\w\\s-]+\"?|[\\w\\s-]+)', ct)\n",
    "\n",
    "        # table_name = table_name_match.group(1) if table_name_match else \"Unknown Table\"\n",
    "        table_name = table_name_match.group(1).strip() if table_name_match else \"Unknown Table\"\n",
    "\n",
    "        # Split the current section at \"INSERT INTO\"\n",
    "        splits = ct.split(\"INSERT INTO\")\n",
    "\n",
    "        # Extract column names and remove the table name from it\n",
    "        # columns = splits[0].replace(table_name, \"\").strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        columns = re.sub(r'^\\s*' + re.escape(table_name), '', splits[0]).strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        columns = \" \".join(columns.split())\n",
    "\n",
    "        # Process INSERT statements\n",
    "        cleaned_table_name = table_name.strip(\"\\\"\")\n",
    "        insert_statements = [split.replace(f'{cleaned_table_name} VALUES', '').strip() for split in splits[1:]]\n",
    "\n",
    "        # Remove parentheses from the INSERT statements\n",
    "        insert_statements = [stmt.replace(\"(\", \"\").replace(\")\", \"\") for stmt in insert_statements]\n",
    "        \n",
    "        # Combine the statements for the current table and append to formatted_ddls\n",
    "        # formatted_ddl = \"# Table: \" + table_name + \"\\n\" + columns + \"\\n\" + \"\\n\".join(insert_statements)\n",
    "        if 'VARBINARY' in ct:\n",
    "            formatted_ddl = table_name + \" (\" + columns + \");\"\n",
    "        else:\n",
    "            formatted_ddl = table_name + \" (\" + columns + \");\\nINSERT INTO \" + table_name + \" VALUES\\n(\" + \")\\n(\".join(insert_statements) +\");\"\n",
    "        formatted_ddls.append(formatted_ddl)\n",
    "\n",
    "    # Join all the formatted sections with newline characters\n",
    "    return \"\\n\".join(formatted_ddls)\n",
    "\n",
    "# Apply the formatting function to the 'ddl' column\n",
    "df_ddl['ddl'] = df_ddl['ddl'].apply(format_ddl)\n",
    "\n",
    "df_question = pd.read_json(JSON_FILE)\n",
    "joined_df = pd.merge(df_ddl, df_question, on=[\"db_id\"])\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT);\n",
      "INSERT INTO CBSA VALUES\n",
      "(10300, 'Adrian, MI', 'Micro')\n",
      "(10380, 'Aguadilla-Isabela, PR', 'Metro')\n",
      "(10420, 'Akron, OH', 'Metro')\n",
      "(10500, 'Albany, GA', 'Metro')\n",
      "(10580, 'Albany-Schenectady-Troy, NY', 'Metro');\n",
      "state (abbreviation TEXT, name TEXT);\n",
      "INSERT INTO state VALUES\n",
      "('AA', 'Armed Forces Americas')\n",
      "('AE', 'Armed Forces Europe')\n",
      "('AK', 'Alaska')\n",
      "('AL', 'Alabama')\n",
      "('AP', 'Armed Forces Pacific');\n",
      "congress (cognress_rep_id TEXT, first_name TEXT, last_name TEXT, CID TEXT, party TEXT, state TEXT, abbreviation TEXT, House TEXT, District INT, land_area FLOAT, FOREIGN KEY abbreviation REFERENCES state abbreviation);\n",
      "INSERT INTO congress VALUES\n",
      "('AK', 'Young', 'Don', 'N00008091', 'Republican', 'Alaska', 'AK', 'House of Repsentatives', NULL, 571951.26)\n",
      "('AK-S1', 'Begich', 'Mark', 'N00009585', 'Democrat', 'Alaska', 'AK', 'Senate', NULL, 570641.0)\n",
      "('AK-S2', 'Murkowski', 'Lisa', 'N00033101', 'Republican', 'Alaska', 'AK', 'Senate', NULL, 570641.0)\n",
      "('AL-1', 'Byrne', 'Bradley', 'N00031938', 'Republican', 'Alabama', 'AL', 'House of Repsentatives', 1, 6066.83)\n",
      "('AL-2', 'Roby', 'Martha', 'N00031177', 'Republican', 'Alabama', 'AL', 'House of Repsentatives', 2, 10141.63);\n",
      "zip_data (zip_code INT, city TEXT, state TEXT, multi_county TEXT, type TEXT, organization TEXT, time_zone TEXT, daylight_savings TEXT, latitude FLOAT, longitude FLOAT, elevation INT, state_fips INT, county_fips INT, region TEXT, division TEXT, population_2020 INT, population_2010 INT, households INT, avg_house_value INT, avg_income_per_household INT, persons_per_household FLOAT, white_population INT, black_population INT, hispanic_population INT, asian_population INT, american_indian_population INT, hawaiian_population INT, other_population INT, male_population INT, female_population INT, median_age FLOAT, male_median_age FLOAT, female_median_age FLOAT, residential_mailboxes INT, business_mailboxes INT, total_delivery_receptacles INT, businesses INT, \"1st_quarter_payroll\" INT, annual_payroll INT, employees INT, water_area FLOAT, land_area FLOAT, single_family_delivery_units INT, multi_family_delivery_units INT, total_beneficiaries INT, retired_workers INT, disabled_workers INT, parents_and_widowed INT, spouses INT, children INT, over_65 INT, monthly_benefits_all INT, monthly_benefits_retired_workers INT, monthly_benefits_widowed INT, CBSA INT, FOREIGN KEY state REFERENCES state abbreviation, FOREIGN KEY CBSA REFERENCES CBSA CBSA);\n",
      "INSERT INTO zip_data VALUES\n",
      "(501, 'Holtsville', 'NY', 'No', 'Unique Post Office', 'I R S Service Center', 'Eastern', 'Yes', 40.817923, -73.045317, 25, 36, 103, 'Northeast', 'Middle Atlantic', 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0, 1, 1, 2, 0, 0, 0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35620)\n",
      "(544, 'Holtsville', 'NY', 'No', 'Unique Post Office', 'Irs Service Center', 'Eastern', 'Yes', 40.788827, -73.039405, 25, 36, 103, 'Northeast', 'Middle Atlantic', 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35620)\n",
      "(601, 'Adjuntas', 'PR', 'No', 'Post Office', NULL, 'Atlantic', 'No', 18.196747, -66.736735, 0, 72, 1, NULL, NULL, 11737, 18570, 6525, 86200, 13092, 2.84, 17479, 663, 18486, 7, 113, 10, 558, 9078, 9492, 35.9, 34.5, 37.1, 4133, 221, 5173, 0, 0, 0, 0, 0.309, 64.348, 2419, 1264, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38660)\n",
      "(602, 'Aguada', 'PR', 'No', 'Post Office', NULL, 'Atlantic', 'No', 18.352927, -67.177532, 0, 72, 3, NULL, NULL, 24263, 41520, 15002, 86300, 16358, 2.76, 36828, 2860, 41265, 42, 291, 32, 2634, 20396, 21124, 37.5, 36.6, 38.5, 8791, 519, 11302, 0, 0, 0, 0, 1.71, 30.621, 5473, 827, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10380)\n",
      "(603, 'Aguadilla', 'PR', 'No', 'Post Office', NULL, 'Atlantic', 'No', 18.458585, -67.129867, 0, 72, 5, NULL, NULL, 40361, 54689, 21161, 122400, 16603, 2.53, 46501, 5042, 53877, 135, 313, 35, 4177, 26597, 28092, 38.2, 36.6, 39.8, 15953, 764, 19186, 0, 0, 0, 0, 0.07, 31.617, 9621, 2947, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10380);\n",
      "alias (zip_code INT, alias TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\n",
      "INSERT INTO alias VALUES\n",
      "(501, 'Holtsville')\n",
      "(544, 'Holtsville')\n",
      "(601, 'Adjuntas')\n",
      "(602, 'Aguada')\n",
      "(603, 'Aguadilla');\n",
      "area_code (zip_code INT, area_code INT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\n",
      "INSERT INTO area_code VALUES\n",
      "(501, 631)\n",
      "(544, 631)\n",
      "(601, 787)\n",
      "(601, 939)\n",
      "(602, 787);\n",
      "avoid (zip_code INT, bad_alias TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\n",
      "INSERT INTO avoid VALUES\n",
      "(501, 'Internal Revenue Service')\n",
      "(544, 'Internal Revenue Service')\n",
      "(601, 'Colinas Del Gigante')\n",
      "(601, 'Jard De Adjuntas')\n",
      "(601, 'URB San Joaquin');\n",
      "country (zip_code INT, county TEXT, state TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code, FOREIGN KEY state REFERENCES state abbreviation);\n",
      "INSERT INTO country VALUES\n",
      "(501, 'SUFFOLK', 'NY')\n",
      "(544, 'SUFFOLK', 'NY')\n",
      "(601, 'ADJUNTAS', 'PR')\n",
      "(602, 'AGUADA', 'PR')\n",
      "(603, 'AGUADILLA', 'PR');\n",
      "zip_congress (zip_code INT, district TEXT, FOREIGN KEY district REFERENCES congress cognress_rep_id, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\n",
      "INSERT INTO zip_congress VALUES\n",
      "(501, 'NY-1')\n",
      "(601, 'PR')\n",
      "(602, 'PR')\n",
      "(603, 'PR')\n",
      "(604, 'PR');\n"
     ]
    }
   ],
   "source": [
    "print(joined_df.iloc[0]['ddl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>db_id</th>\n",
       "      <th>ideal_assistant_response</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>address</td>\n",
       "      <td>SELECT SUM(T1.households) FROM zip_data AS T1 ...</td>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>address</td>\n",
       "      <td>SELECT T1.zip_code FROM zip_data AS T1 INNER J...</td>\n",
       "      <td>2097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>address</td>\n",
       "      <td>SELECT SUM(T1.male_population) FROM zip_data A...</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>address</td>\n",
       "      <td>SELECT COUNT(T1.zip_code) FROM zip_data AS T1 ...</td>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...</td>\n",
       "      <td>address</td>\n",
       "      <td>SELECT T1.zip_code FROM zip_data AS T1 INNER J...</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         user_prompt    db_id  \\\n",
       "0  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...  address   \n",
       "1  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...  address   \n",
       "2  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...  address   \n",
       "3  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...  address   \n",
       "4  CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT...  address   \n",
       "\n",
       "                            ideal_assistant_response  tokens  \n",
       "0  SELECT SUM(T1.households) FROM zip_data AS T1 ...    2089  \n",
       "1  SELECT T1.zip_code FROM zip_data AS T1 INNER J...    2097  \n",
       "2  SELECT SUM(T1.male_population) FROM zip_data A...    2092  \n",
       "3  SELECT COUNT(T1.zip_code) FROM zip_data AS T1 ...    2091  \n",
       "4  SELECT T1.zip_code FROM zip_data AS T1 INNER J...    2100  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    joined_df.apply(\n",
    "        lambda x: make_x(\n",
    "            x[\"ddl\"]\n",
    "            + \"\\n## The user has asked:\\n\"\n",
    "            + x[\"question\"]\n",
    "            + \"\\nNOTE: \"\n",
    "            + x['evidence'],\n",
    "            x[\"db_id\"],\n",
    "            x['SQL']\n",
    "        ),\n",
    "        axis=1,\n",
    "    ).tolist()\n",
    ")\n",
    "df.columns = [\n",
    "    \"user_prompt\",\n",
    "    \"db_id\",\n",
    "    \"ideal_assistant_response\"\n",
    "]\n",
    "\n",
    "# Drop the row that contains the example gold sql\n",
    "example_gold_sql= \"SELECT T1.Rating, COUNT(T2.Sentiment_Polarity) FROM playstore AS T1 INNER JOIN user_reviews AS T2 ON T1.App = T2.App WHERE T1.App = 'Dragon Ball Legends' AND CAST(Sentiment_Polarity AS INTEGER) < -0.5\"\n",
    "df = df[df['ideal_assistant_response'] != example_gold_sql]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['tokens'] = df.apply(compute_tokens, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>db_id</th>\n",
       "      <th>ideal_assistant_response</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_prompt, db_id, ideal_assistant_response, tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify we dropped the example gold sql\n",
    "df[df['ideal_assistant_response'] == example_gold_sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>db_id</th>\n",
       "      <th>ideal_assistant_response</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>SELECT CAST(SUM(T2.height) AS REAL) / COUNT(*)...</td>\n",
       "      <td>3867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>SELECT SUM(CASE WHEN T1.year = 2006 THEN CAST(...</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>SELECT T2.nameGiven, T2.lastName, T2.birthYear...</td>\n",
       "      <td>3855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>SELECT DISTINCT T3.firstNHL - T1.year, T3.name...</td>\n",
       "      <td>3851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...</td>\n",
       "      <td>hockey</td>\n",
       "      <td>SELECT DISTINCT T1.pos FROM Master AS T1 INNER...</td>\n",
       "      <td>3840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            user_prompt   db_id  \\\n",
       "2618  AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...  hockey   \n",
       "2603  AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...  hockey   \n",
       "2634  AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...  hockey   \n",
       "2623  AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...  hockey   \n",
       "2563  AwardsMisc (name TEXT, ID TEXT, award TEXT, ye...  hockey   \n",
       "\n",
       "                               ideal_assistant_response  tokens  \n",
       "2618  SELECT CAST(SUM(T2.height) AS REAL) / COUNT(*)...    3867  \n",
       "2603  SELECT SUM(CASE WHEN T1.year = 2006 THEN CAST(...    3861  \n",
       "2634  SELECT T2.nameGiven, T2.lastName, T2.birthYear...    3855  \n",
       "2623  SELECT DISTINCT T3.firstNHL - T1.year, T3.name...    3851  \n",
       "2563  SELECT DISTINCT T1.pos FROM Master AS T1 INNER...    3840  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='tokens', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awards_players (playerID TEXT, award TEXT, year INT, lgID TEXT, note TEXT, pos TEXT, FOREIGN KEY playerID REFERENCES players playerID);\n",
      "INSERT INTO awards_players VALUES\n",
      "('abdulka01', 'All-Defensive Second Team', 1969, 'NBA', NULL, NULL)\n",
      "('abdulka01', 'All-NBA Second Team', 1969, 'NBA', NULL, 'C')\n",
      "('abdulka01', 'Rookie of the Year', 1969, 'NBA', NULL, NULL)\n",
      "('abdulka01', 'All-Defensive Second Team', 1970, 'NBA', NULL, NULL)\n",
      "('abdulka01', 'All-NBA First Team', 1970, 'NBA', NULL, 'C');\n",
      "coaches (coachID TEXT, year INT, tmID TEXT, lgID TEXT, stint INT, won INT, lost INT, post_wins INT, post_losses INT, FOREIGN KEY tmID, year REFERENCES teams tmID, year);\n",
      "INSERT INTO coaches VALUES\n",
      "('adelmri01', 1988, 'POR', 'NBA', 2, 14, 21, 0, 3)\n",
      "('adelmri01', 1989, 'POR', 'NBA', 1, 59, 23, 12, 9)\n",
      "('adelmri01', 1990, 'POR', 'NBA', 1, 63, 19, 9, 7)\n",
      "('adelmri01', 1991, 'POR', 'NBA', 1, 57, 25, 13, 8)\n",
      "('adelmri01', 1992, 'POR', 'NBA', 1, 51, 31, 1, 3);\n",
      "draft (id INT, draftYear INT, draftRound INT, draftSelection INT, draftOverall INT, tmID TEXT, firstName TEXT, lastName TEXT, suffixName TEXT, playerID TEXT, draftFrom TEXT, lgID TEXT, FOREIGN KEY tmID, draftYear REFERENCES teams tmID, year);\n",
      "INSERT INTO draft VALUES\n",
      "(1, 1967, 0, 0, 0, 'ANA', 'Darrell', 'Hardy', NULL, 'hardyda01', 'Baylor', 'ABA')\n",
      "(2, 1967, 0, 0, 0, 'ANA', 'Bob', 'Krulish', NULL, NULL, 'Pacific', 'ABA')\n",
      "(3, 1967, 0, 0, 0, 'ANA', 'Bob', 'Lewis', NULL, 'lewisbo01', 'North Carolina', 'ABA')\n",
      "(4, 1967, 0, 0, 0, 'ANA', 'Mike', 'Lynn', NULL, 'lynnmi01', 'UCLA', 'ABA')\n",
      "(5, 1967, 0, 0, 0, 'ANA', 'Tom', 'Workman', NULL, 'workmto01', 'Seattle', 'ABA');\n",
      "player_allstar (playerID TEXT, last_name TEXT, first_name TEXT, season_id INT, conference TEXT, league_id TEXT, games_played INT, minutes INT, points INT, o_rebounds INT, d_rebounds INT, rebounds INT, assists INT, steals INT, blocks INT, turnovers INT, personal_fouls INT, fg_attempted INT, fg_made INT, ft_attempted INT, ft_made INT, three_attempted INT, three_made INT, FOREIGN KEY playerID REFERENCES players playerID);\n",
      "INSERT INTO player_allstar VALUES\n",
      "('abdulka01', 'Abdul-Jabbar', 'Kareem', 1969, 'East', 'NBA', 1, 18, 10, NULL, NULL, 11, 4, NULL, NULL, NULL, NULL, 8, 4, 2, 2, NULL, NULL)\n",
      "('abdulka01', 'Abdul-Jabbar', 'Kareem', 1970, 'West', 'NBA', 1, 30, 19, NULL, NULL, 14, 1, NULL, NULL, NULL, NULL, 16, 8, 4, 3, NULL, NULL)\n",
      "('abdulka01', 'Abdul-Jabbar', 'Kareem', 1971, 'West', 'NBA', 1, 19, 12, NULL, NULL, 7, 2, NULL, NULL, NULL, NULL, 10, 5, 2, 2, NULL, NULL)\n",
      "('abdulka01', 'Abdul-Jabbar', 'Kareem', 1972, 'West', 'NBA', 1, 98, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL)\n",
      "('abdulka01', 'Abdul-Jabbar', 'Kareem', 1973, 'West', 'NBA', 1, 23, 14, NULL, NULL, 8, 6, NULL, NULL, NULL, NULL, 11, 7, 0, 0, NULL, NULL);\n",
      "players (playerID TEXT, useFirst TEXT, firstName TEXT, middleName TEXT, lastName TEXT, nameGiven TEXT, fullGivenName TEXT, nameSuffix TEXT, nameNick TEXT, pos TEXT, firstseason INT, lastseason INT, height FLOAT, weight INT, college TEXT, collegeOther TEXT, birthDate DATE, birthCity TEXT, birthState TEXT, birthCountry TEXT, highSchool TEXT, hsCity TEXT, hsState TEXT, hsCountry TEXT, deathDate DATE, race TEXT);\n",
      "INSERT INTO players VALUES\n",
      "('abdelal01', 'Alaa', 'Alaa', NULL, 'Abdelnaby', NULL, NULL, NULL, NULL, 'F-C', 0, 0, 82.0, 240, 'Duke', NULL, '1968-06-24', 'Cairo', NULL, 'EGY', 'Bloomfield Senior', 'Bloomfield', 'NJ', 'USA', '0000-00-00', 'B')\n",
      "('abdulka01', 'Kareem', 'Kareem', NULL, 'Abdul-Jabbar', NULL, 'Ferdinand Lewis Alcindor, Jr.', NULL, 'Lew, Cap', 'C', 0, 0, 85.0, 225, 'UCLA', NULL, '1947-04-16', 'New York', 'NY', 'USA', 'Power Memorial', 'New York', 'NY', 'USA', '0000-00-00', 'B')\n",
      "('abdulma01', 'Mahdi', 'Mahdi', NULL, 'Abdul-Rahman', NULL, 'Walter Raphael Hazzard, Jr.', NULL, 'Walt', 'G', 0, 0, 74.0, 185, 'UCLA', 'Santa Monica City', '1942-04-15', 'Wilmington', 'DE', 'USA', 'Overbrook / Moton', 'Philadelphia / Easton', 'PA / MD', 'USA', '2011-11-18', 'B')\n",
      "('abdulma02', 'Mahmoud', 'Mahmoud', NULL, 'Abdul-Rauf', NULL, 'Chris Wayne Jackson', NULL, NULL, 'G', 0, 0, 73.0, 162, 'Louisiana State', NULL, '1969-03-09', 'Gulfport', 'MS', 'USA', 'Gulfport', 'Gulfport', 'MS', 'USA', '0000-00-00', 'B')\n",
      "('abdulta01', 'Tariq', 'Tariq', NULL, 'Abdul-Wahad', NULL, 'Olivier Michael Saint-Jean', NULL, NULL, 'G-F', 0, 0, 78.0, 223, 'San Jose State', 'Michigan', '1974-11-03', 'Maisons Alfort', NULL, 'FRA', 'Lycee Aristide Briand', 'Evreux', NULL, 'FRA', '0000-00-00', 'B');\n",
      "teams (year INT, lgID TEXT, tmID TEXT, franchID TEXT, confID TEXT, divID TEXT, \"rank\" INT, confRank INT, playoff TEXT, name TEXT, o_fgm INT, o_ftm INT, o_pts INT, d_pts INT, homeWon INT, homeLost INT, awayWon INT, awayLost INT, won INT, lost INT, games INT, arena TEXT);\n",
      "INSERT INTO teams VALUES\n",
      "(1937, 'NBL', 'AFS', 'AFS', NULL, 'EA', 1, 0, 'CF', 'Akron Firestone Non-Skids', 249, 183, 681, 578, 8, 1, 5, 3, 14, 4, 18, NULL)\n",
      "(1937, 'NBL', 'AGW', 'AGW', NULL, 'EA', 2, 0, 'WC', 'Akron Goodyear Wingfoots', 243, 159, 645, 498, 8, 1, 5, 4, 13, 5, 18, NULL)\n",
      "(1937, 'NBL', 'BFB', 'BFB', NULL, 'EA', 4, 0, NULL, 'Buffalo Bisons', 108, 46, 262, 275, 2, 2, 1, 4, 3, 6, 9, NULL)\n",
      "(1937, 'NBL', 'CNC', 'CNC', NULL, 'WE', 5, 0, NULL, 'Richmond King Clothiers/Cincinnati Comellos', 110, 42, 262, 338, 3, 1, 0, 5, 3, 7, 10, NULL)\n",
      "(1937, 'NBL', 'COL', 'COL', NULL, 'EA', 6, 0, NULL, 'Columbus Athletic Supply', 109, 64, 282, 426, 1, 3, 0, 7, 1, 12, 13, NULL);\n",
      "\"awards_coaches\" (id INT, year INT, coachID TEXT, award TEXT, lgID TEXT, note TEXT, FOREIGN KEY coachID, year REFERENCES coaches coachID, year);\n",
      "INSERT INTO \"awards_coaches\" VALUES\n",
      "(1, 1962, 'gallaha01', 'NBA Coach of the Year', 'NBA', NULL)\n",
      "(2, 1963, 'hannual01', 'NBA Coach of the Year', 'NBA', NULL)\n",
      "(3, 1964, 'auerbre01', 'NBA Coach of the Year', 'NBA', NULL)\n",
      "(4, 1965, 'schaydo01', 'NBA Coach of the Year', 'NBA', NULL)\n",
      "(5, 1966, 'kerrjo01', 'NBA Coach of the Year', 'NBA', NULL);\n",
      "\"players_teams\" (id INT, playerID TEXT, year INT, stint INT, tmID TEXT, lgID TEXT, GP INT, GS INT, minutes INT, points INT, oRebounds INT, dRebounds INT, rebounds INT, assists INT, steals INT, blocks INT, turnovers INT, PF INT, fgAttempted INT, fgMade INT, ftAttempted INT, ftMade INT, threeAttempted INT, threeMade INT, PostGP INT, PostGS INT, PostMinutes INT, PostPoints INT, PostoRebounds INT, PostdRebounds INT, PostRebounds INT, PostAssists INT, PostSteals INT, PostBlocks INT, PostTurnovers INT, PostPF INT, PostfgAttempted INT, PostfgMade INT, PostftAttempted INT, PostftMade INT, PostthreeAttempted INT, PostthreeMade INT, note TEXT, FOREIGN KEY tmID, year REFERENCES teams tmID, year);\n",
      "INSERT INTO \"players_teams\" VALUES\n",
      "(1, 'abdelal01', 1990, 1, 'POR', 'NBA', 43, 0, 290, 135, 27, 62, 89, 12, 4, 12, 22, 39, 116, 55, 44, 25, 0, 0, 5, 0, 13, 4, 1, 2, 3, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, NULL)\n",
      "(2, 'abdelal01', 1991, 1, 'POR', 'NBA', 71, 0, 934, 432, 81, 179, 260, 30, 25, 17, 66, 132, 361, 178, 101, 76, 0, 0, 8, 0, 25, 12, 0, 4, 4, 2, 0, 0, 2, 4, 10, 5, 4, 2, 0, 0, NULL)\n",
      "(3, 'abdelal01', 1992, 1, 'MIL', 'NBA', 12, 0, 159, 64, 12, 25, 37, 10, 6, 4, 0, 24, 56, 26, 16, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NULL)\n",
      "(4, 'abdelal01', 1992, 2, 'BOS', 'NBA', 63, 0, 1152, 514, 114, 186, 300, 17, 19, 22, 97, 165, 417, 219, 100, 76, 0, 0, 4, 0, 68, 22, 2, 11, 13, 1, 0, 1, 9, 7, 24, 11, 0, 0, 0, 0, NULL)\n",
      "(5, 'abdelal01', 1993, 1, 'BOS', 'NBA', 13, 0, 159, 64, 12, 34, 46, 3, 2, 3, 17, 20, 55, 24, 25, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NULL);\n",
      "\"series_post\" (id INT, year INT, round TEXT, series TEXT, tmIDWinner TEXT, lgIDWinner TEXT, tmIDLoser TEXT, lgIDLoser TEXT, W INT, L INT, FOREIGN KEY tmIDWinner, year REFERENCES teams tmID, year, FOREIGN KEY tmIDLoser, year REFERENCES teams tmID, year);\n",
      "INSERT INTO \"series_post\" VALUES\n",
      "(1, 1946, 'F', 'O', 'PHW', 'NBA', 'CHS', 'NBA', 4, 1)\n",
      "(2, 1946, 'QF', 'M', 'NYK', 'NBA', 'CLR', 'NBA', 2, 1)\n",
      "(3, 1946, 'QF', 'M', 'PHW', 'NBA', 'STB', 'NBA', 2, 1)\n",
      "(4, 1946, 'SF', 'N', 'PHW', 'NBA', 'NYK', 'NBA', 2, 0)\n",
      "(5, 1946, 'SF', 'N', 'CHS', 'NBA', 'WSC', 'NBA', 4, 2);\n",
      "## The user has asked:\n",
      "What is the difference in the average age of players when they are drafted in the ABA vs when they are drafted in the NBA between the years 1970 and 1970?\n",
      "NOTE: ABA refers to lgID = 'ABA'; NBA refers to lgID = 'NBA'; between the years 1970 and 1970 refers to draftYear between 1970 and 1970; difference = subtract(avg(subtract(1970, year(birthDate)) where lgID = 'ABA'), avg(subtract(1970, year(birthDate)) where lgID = 'NBA'))\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[5066]['user_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.351176"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].sum() / 1000.0 * .008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = \"fine_tuning_bird_qna_take_two_training_data.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as outfile:\n",
    "    for row in df.itertuples():\n",
    "        json.dump({\"messages\": [{\"role\": \"user\", \"content\": row.user_prompt}, \n",
    "                                {\"role\": \"assistant\", \"content\": row.ideal_assistant_response}]}, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 9424\n",
      "First example:\n",
      "{'role': 'user', 'content': 'CBSA (CBSA INT, CBSA_name TEXT, CBSA_type TEXT);\\nINSERT INTO CBSA VALUES\\n(10300, \\'Adrian, MI\\', \\'Micro\\')\\n(10380, \\'Aguadilla-Isabela, PR\\', \\'Metro\\')\\n(10420, \\'Akron, OH\\', \\'Metro\\')\\n(10500, \\'Albany, GA\\', \\'Metro\\')\\n(10580, \\'Albany-Schenectady-Troy, NY\\', \\'Metro\\');\\nstate (abbreviation TEXT, name TEXT);\\nINSERT INTO state VALUES\\n(\\'AA\\', \\'Armed Forces Americas\\')\\n(\\'AE\\', \\'Armed Forces Europe\\')\\n(\\'AK\\', \\'Alaska\\')\\n(\\'AL\\', \\'Alabama\\')\\n(\\'AP\\', \\'Armed Forces Pacific\\');\\ncongress (cognress_rep_id TEXT, first_name TEXT, last_name TEXT, CID TEXT, party TEXT, state TEXT, abbreviation TEXT, House TEXT, District INT, land_area FLOAT, FOREIGN KEY abbreviation REFERENCES state abbreviation);\\nINSERT INTO congress VALUES\\n(\\'AK\\', \\'Young\\', \\'Don\\', \\'N00008091\\', \\'Republican\\', \\'Alaska\\', \\'AK\\', \\'House of Repsentatives\\', NULL, 571951.26)\\n(\\'AK-S1\\', \\'Begich\\', \\'Mark\\', \\'N00009585\\', \\'Democrat\\', \\'Alaska\\', \\'AK\\', \\'Senate\\', NULL, 570641.0)\\n(\\'AK-S2\\', \\'Murkowski\\', \\'Lisa\\', \\'N00033101\\', \\'Republican\\', \\'Alaska\\', \\'AK\\', \\'Senate\\', NULL, 570641.0)\\n(\\'AL-1\\', \\'Byrne\\', \\'Bradley\\', \\'N00031938\\', \\'Republican\\', \\'Alabama\\', \\'AL\\', \\'House of Repsentatives\\', 1, 6066.83)\\n(\\'AL-2\\', \\'Roby\\', \\'Martha\\', \\'N00031177\\', \\'Republican\\', \\'Alabama\\', \\'AL\\', \\'House of Repsentatives\\', 2, 10141.63);\\nzip_data (zip_code INT, city TEXT, state TEXT, multi_county TEXT, type TEXT, organization TEXT, time_zone TEXT, daylight_savings TEXT, latitude FLOAT, longitude FLOAT, elevation INT, state_fips INT, county_fips INT, region TEXT, division TEXT, population_2020 INT, population_2010 INT, households INT, avg_house_value INT, avg_income_per_household INT, persons_per_household FLOAT, white_population INT, black_population INT, hispanic_population INT, asian_population INT, american_indian_population INT, hawaiian_population INT, other_population INT, male_population INT, female_population INT, median_age FLOAT, male_median_age FLOAT, female_median_age FLOAT, residential_mailboxes INT, business_mailboxes INT, total_delivery_receptacles INT, businesses INT, \"1st_quarter_payroll\" INT, annual_payroll INT, employees INT, water_area FLOAT, land_area FLOAT, single_family_delivery_units INT, multi_family_delivery_units INT, total_beneficiaries INT, retired_workers INT, disabled_workers INT, parents_and_widowed INT, spouses INT, children INT, over_65 INT, monthly_benefits_all INT, monthly_benefits_retired_workers INT, monthly_benefits_widowed INT, CBSA INT, FOREIGN KEY state REFERENCES state abbreviation, FOREIGN KEY CBSA REFERENCES CBSA CBSA);\\nINSERT INTO zip_data VALUES\\n(501, \\'Holtsville\\', \\'NY\\', \\'No\\', \\'Unique Post Office\\', \\'I R S Service Center\\', \\'Eastern\\', \\'Yes\\', 40.817923, -73.045317, 25, 36, 103, \\'Northeast\\', \\'Middle Atlantic\\', 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0, 1, 1, 2, 0, 0, 0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35620)\\n(544, \\'Holtsville\\', \\'NY\\', \\'No\\', \\'Unique Post Office\\', \\'Irs Service Center\\', \\'Eastern\\', \\'Yes\\', 40.788827, -73.039405, 25, 36, 103, \\'Northeast\\', \\'Middle Atlantic\\', 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35620)\\n(601, \\'Adjuntas\\', \\'PR\\', \\'No\\', \\'Post Office\\', NULL, \\'Atlantic\\', \\'No\\', 18.196747, -66.736735, 0, 72, 1, NULL, NULL, 11737, 18570, 6525, 86200, 13092, 2.84, 17479, 663, 18486, 7, 113, 10, 558, 9078, 9492, 35.9, 34.5, 37.1, 4133, 221, 5173, 0, 0, 0, 0, 0.309, 64.348, 2419, 1264, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38660)\\n(602, \\'Aguada\\', \\'PR\\', \\'No\\', \\'Post Office\\', NULL, \\'Atlantic\\', \\'No\\', 18.352927, -67.177532, 0, 72, 3, NULL, NULL, 24263, 41520, 15002, 86300, 16358, 2.76, 36828, 2860, 41265, 42, 291, 32, 2634, 20396, 21124, 37.5, 36.6, 38.5, 8791, 519, 11302, 0, 0, 0, 0, 1.71, 30.621, 5473, 827, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10380)\\n(603, \\'Aguadilla\\', \\'PR\\', \\'No\\', \\'Post Office\\', NULL, \\'Atlantic\\', \\'No\\', 18.458585, -67.129867, 0, 72, 5, NULL, NULL, 40361, 54689, 21161, 122400, 16603, 2.53, 46501, 5042, 53877, 135, 313, 35, 4177, 26597, 28092, 38.2, 36.6, 39.8, 15953, 764, 19186, 0, 0, 0, 0, 0.07, 31.617, 9621, 2947, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10380);\\nalias (zip_code INT, alias TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\\nINSERT INTO alias VALUES\\n(501, \\'Holtsville\\')\\n(544, \\'Holtsville\\')\\n(601, \\'Adjuntas\\')\\n(602, \\'Aguada\\')\\n(603, \\'Aguadilla\\');\\narea_code (zip_code INT, area_code INT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\\nINSERT INTO area_code VALUES\\n(501, 631)\\n(544, 631)\\n(601, 787)\\n(601, 939)\\n(602, 787);\\navoid (zip_code INT, bad_alias TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\\nINSERT INTO avoid VALUES\\n(501, \\'Internal Revenue Service\\')\\n(544, \\'Internal Revenue Service\\')\\n(601, \\'Colinas Del Gigante\\')\\n(601, \\'Jard De Adjuntas\\')\\n(601, \\'URB San Joaquin\\');\\ncountry (zip_code INT, county TEXT, state TEXT, FOREIGN KEY zip_code REFERENCES zip_data zip_code, FOREIGN KEY state REFERENCES state abbreviation);\\nINSERT INTO country VALUES\\n(501, \\'SUFFOLK\\', \\'NY\\')\\n(544, \\'SUFFOLK\\', \\'NY\\')\\n(601, \\'ADJUNTAS\\', \\'PR\\')\\n(602, \\'AGUADA\\', \\'PR\\')\\n(603, \\'AGUADILLA\\', \\'PR\\');\\nzip_congress (zip_code INT, district TEXT, FOREIGN KEY district REFERENCES congress cognress_rep_id, FOREIGN KEY zip_code REFERENCES zip_data zip_code);\\nINSERT INTO zip_congress VALUES\\n(501, \\'NY-1\\')\\n(601, \\'PR\\')\\n(602, \\'PR\\')\\n(603, \\'PR\\')\\n(604, \\'PR\\');\\n## The user has asked:\\nWhat is the total number of households in Arecibo county?\\nNOTE: \"ARECIBO\" is the county; total number of households refer to sum(households)'}\n",
      "{'role': 'assistant', 'content': \"SELECT SUM(T1.households) FROM zip_data AS T1 INNER JOIN country AS T2 ON T1.zip_code = T2.zip_code WHERE T2.county = 'ARECIBO'\"}\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 9424\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 2, 2\n",
      "mean / median: 2.0, 2.0\n",
      "p5 / p95: 2.0, 2.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 256, 4015\n",
      "mean / median: 1810.1644736842106, 1686.0\n",
      "p5 / p95: 609.3000000000001, 3326.7000000000007\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 4, 278\n",
      "mean / median: 50.65757640067912, 49.0\n",
      "p5 / p95: 18.0, 80.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~17058990 tokens that will be charged for during training\n",
      "By default, you'll train for 2 epochs on this dataset\n",
      "By default, you'll be charged for ~34117980 tokens\n",
      "This will cost ~$272.94 to train\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "PRICE_PER_1K_TOKENS = 0.008\n",
    "\n",
    "print(f\"This will cost ~${n_epochs * n_billing_tokens_in_dataset / 1000 * PRICE_PER_1K_TOKENS:.2f} to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-yt2tzTmOhKmbL4clWJ9ZmWAR at 0x14d770710> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-yt2tzTmOhKmbL4clWJ9ZmWAR\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 47915791,\n",
       "  \"created_at\": 1698802653,\n",
       "  \"status\": \"processed\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_response = openai.File.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-yt2tzTmOhKmbL4clWJ9ZmWAR'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id = upload_response.id\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-EPDJqgOoHYfRBV1s727wPwIr at 0x1274d5d90> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-EPDJqgOoHYfRBV1s727wPwIr\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1698802777,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-ePmgB4qVo14GgUKdUQci6IGz\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-yt2tzTmOhKmbL4clWJ9ZmWAR\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": \"auto\"\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\", suffix=\"bird_qna_take_two\")\n",
    "fine_tune_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-EPDJqgOoHYfRBV1s727wPwIr at 0x1274d7d10> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-EPDJqgOoHYfRBV1s727wPwIr\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1698802777,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-ePmgB4qVo14GgUKdUQci6IGz\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"validating_files\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-yt2tzTmOhKmbL4clWJ9ZmWAR\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": \"auto\"\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the state of a fine-tune\n",
    "openai.FineTuningJob.retrieve(\"ftjob-EPDJqgOoHYfRBV1s727wPwIr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
